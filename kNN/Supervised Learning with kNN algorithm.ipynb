{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Supervised Learning - k-Nearest Neighbor (kNN) Algorithm \n",
    "\n",
    "In this lab, we will make first steps in doing supervised learning. in particular, we will learn about the k-Nearest Neighbor (kNN) algorithm. kNN uses a simple idea: \"you are what your neighbors are\". This idea work quite well in data science. In the first part of the lab, we will cover some background needed to understand the kNN algorithm. In the second part, you will be asked to apply your knowledge on another data set. \n",
    "\n",
    "## Lab 5.A: kNN Tutorial with Questions (30% of grade)\n",
    "\n",
    "As usual, let us start by importing the needed libraries. We will continue using the sklearn library, which implements many of the most popular data science algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the Iris data set using a sklearn function `load_iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html, `iris` is an object with attributes `data` (a 150x4 matrix, where $i$-th row are 4 attributes of the $i$-th flower), `feature_names` (the names of the 4 attributes -- remember that in data science \"attribute\" and \"feature\" means the same thing), `target` (a vector of length 150, where $i$-th number is the type of the $i$-th flower -- in data science people often say \"label\" instead of \"target\"), `target_names` (these are strings explaining what each of the 3 types of flowers are), and `DESCR` (giving some information about the Iris data set). Let us list them all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)\n",
    "print(iris.data)\n",
    "print(iris.feature_names)\n",
    "print(iris.target)\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the attributes of the second flower are `[4.9, 3.0, 1.4, 0.2]`, which means its `sepal_length` is 4.9 cm, `sepal_width` is 3.0 cm, `petal_length` is 1.4 cm, and `petal_width` is 0.2 cm. We will write it matematically as $x_2 = [x_{21}, x_{22}, x_{23}, x_{24}] = [4.9, 3.0, 1.4, 0.2]$. We see that its `target` is 0, which means the type of this iris is `setosa`. We will write it matematically as $y_2 = 0$. All this information was obtained by real botanists who studied iris flowers trying to understand the physical measurements that discriminate between the 3 different types of those flowers.\n",
    "\n",
    "In data science, people like to denote this data set as $D_{Iris} = \\{(x_i, y_i), i = 1, 2 ... 150\\}$, meaning that data set $D_{Iris}$ is a set of 150 labeled examples $(x_i, y_i)$. An alternative is to write $D_{Iris} = \\{X_{Iris}, Y_{Iris}\\}$.\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning is a game with the following objective. You are given the iris data set $D_{Iris}$ where you know 4 attributes and target values for 150 irises and your objective is to come up with a computer program that predicts a type of any iris flower given the values of its 4 attributes. Written in pseudocode, this is what you have to do:\n",
    "\n",
    "`predictor = create(algorithm_type, D)\n",
    "y_new = predictor(x_new)`\n",
    "\n",
    "In the first line, you are running a `create` function that takes as input data set `D` and the name of a supervised learning algorithm `algorithm_type` and produces as an output a computer program `predictor`. In the second line, you are using `predictor` to predict the label (`y_new` value) for a flower whose attributes are given by `x_new`.\n",
    "\n",
    "### kNN Algorithm\n",
    "kNN is a popular supervised learning algorithm that allows us to create `predictor`. The idea of kNN is that the label of flower `x_new` depends on labels of flowers in its neighborhood. In particular, kNN finds the distance between `x_new` and every example `x` in data set `D`. Then, it looks at the label `y` of k examples which are the closest to `x_new`. The predicted label `y_new` is obtained as the most common label in the group of the k nearest neighbors.\n",
    "\n",
    "**Parameter choice**. We need to make a few decisions when running kNN. The most important is the choice of `k`. If `k = 1`, then we are looking only at the hearest neighbor and it might not be a good idea if we are dealing with noisy data. If `k` is very large, then we might be counting far neighbors that might have different properties. Other decisions include the choice of distance metric (Euclidean is the standard one) and the choice whether to weight closer neighbors more than the farther ones.\n",
    "\n",
    "**Accuracy**. When deciding which parameters to pick or which supervised learning algorithm to use (there are popular algorithms other than kNN), the question is how to measure which choice is better. The answer is to check if `predictor` provides accurate prediction. Given a data set `D`, a typical way to check accuracy is to randomly split `D` into two data sets, `D_train` and `D_test`. Then, `predictor` is created/trained using `D_train` data set and its accuracy is checked using `D_test`. In particular, we use `predictor` to predict label of every example from `D_test` and compare it with the true labels. The percentage of the correct guesses on `D_test` is reported as accuracy of `predictor`.\n",
    "\n",
    "## kNN Demo\n",
    "The following piece of code is taken from:\n",
    "http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py. Let us run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris.data is of type <class 'numpy.ndarray'>\n",
      "The dataset has this dimension  (150, 4)\n",
      "Iris.target is of type <class 'numpy.ndarray'>\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Iris.data is of type\", type(iris.data))\n",
    "print(\"The dataset has this dimension \", iris.data.shape)\n",
    "print(\"Iris.target is of type\", type(iris.target))\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris.target should have a dimension of 1 because it is the label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 3   # how many nearest neighbors are consulted\n",
    "\n",
    "# get all the rows with the first 2 columns\n",
    "X = iris.data[:, [0,1]]  # we only take the first two features. We could\n",
    "\n",
    "# \n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "\n",
    "# This is the predictor = create(algorithm_type, D) step as mentioned\n",
    "# in the text above\n",
    "clf = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# This is the y_new = predictor(x_new) step as mentioned above in the text\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.figure(figsize=(6,3))   # this makes both axis equal \n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i, weights = '%s')\" % (k, 'uniform'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting figure shows the predictions of kNN when $k=1$. If `x_new` is in the blue region, the prediction will be the blue class. From this picture, we can observe a small blue blobs inside the predominantly gray area. This is because the nearest neighbor in this area is the blue example. \n",
    "\n",
    "**Question 1**. Change value of k to 3 and observe if there is any difference. Discuss what you see and why.\n",
    "\n",
    "**Question 2**. Change k to an even higher value, let us say to 25. What do we see now? Discuss.\n",
    "\n",
    "**Queston 3**. In the line that creates `clf` change weights='uniform' to weights='distance'. Check the documentation or google to understand what it means. Explain. Run the code and discuss if you see any difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** After changing k to 3, I see that the green region is more flatten out. It takes up more space and it expands leftward to the red region and rightward to the blue region. Thus, the green region is covering more of the green points versus when k was set to 25. \n",
    "\n",
    "The boundaries are more refined to cover the points with the same color. \n",
    "\n",
    "I think we observed these differences is because of the fact that the number of neighbors is smaller so the algorithm only cares about the most nearest one. When we set k to 25, the algorithm might pick further neighbors with different properties and if those further neighbors dominate as a majority when it comes to predicting the label, this might have a more siginificance and therefore, the figure is not as refined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:** As k being set to a higher value, the figure is less refined. I observe that the green region is swallowing up more blue dots as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:** \n",
    "\n",
    "After consulting some online documentation, this means that the algorithm gives more weights to closer neighbors than farther neighbors when weights=distance. For weights=uniform, the algorithm gives the same weight to all k-closest neighbors regardless of its distance. \n",
    "\n",
    "By setting clf to distance, I observe that the figure is more refined. The blue dots that were covered by the green region are now being covered by the blue region. This might means that the algorithm's accuracy has gone up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "  As mentioned above, the typical mechanism for testing accuracy of a `predictor` is to split the data randomly into training and testing, train `predictor` on training data and test its performance on test data. Let us see how it can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the test set will be 33% of the original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**. What is the size of the resulting objects?\n",
    "\n",
    "**Answer 4:** The test set should be 33% of the original iris dataset.\n",
    "\n",
    "Now that we created training and test sets, we can train a kNN classifier using the training data. Before moving forward, let us take a second and take a look at the documentation for kNN implementation in sklearn: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\n",
    "\n",
    "Let us train the kNN predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "                     weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "k = 1   # number of nearest neighbors\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "predictor.fit(X_train, y_train);\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained `predictor` we can use it to provide predictions on any example `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.5 3.2]\n",
      " [6.3 3.3]\n",
      " [6.9 3.1]\n",
      " [4.4 3.2]]\n",
      "[(1, 2), (1, 2), (1, 2), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# select the first 4 test examples\n",
    "i = [0,1,2,3]\n",
    "x = X_test[i,:]\n",
    "print(x)\n",
    "# predict its label\n",
    "yhat = predictor.predict(x)\n",
    "\n",
    "# compare predicted and true labels\n",
    "\n",
    "# zip matches an item from one list with another.\n",
    "# both item must have the same index\n",
    "print(list(zip(yhat,y_test[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**. Did your kNN predictor do a good job in predicting labels of the first 4 test examples? \n",
    "\n",
    "**Answer 5:** The kNN achieves a 100% accuracy when predicting the labels of the first 4 test examples.\n",
    "\n",
    "**Question 6**. Write a piece of code that calculates the accuracy on those 4 test examples (number of correct guesses divided by the total number of guesses\n",
    "\n",
    "**Question 7**. Find the predictions on all test examples in `X_test` and calculate the accuracy using your code from *Question 6*.\n",
    "\n",
    "Pay attention that methods in sklearn.neighbors.KNeighborsClassifier allow you to test the accuracy in a faster way (you should not use it to answer Questions 6 and 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.0\n"
     ]
    }
   ],
   "source": [
    "# Answer 6\n",
    "# predictions and true_labels must be arrays\n",
    "# they can either be NP arrays or of list type\n",
    "def accuracy_rate(predictions, true_labels):\n",
    "    right_count = 0\n",
    "    for i in range(len(predictions)):\n",
    "        predict_label = predictions[i]\n",
    "        true_label = true_labels[i]\n",
    "        \n",
    "        if predict_label == true_label:\n",
    "            right_count += 1\n",
    "    score = (right_count / len(predictions)) * 100.00\n",
    "    return score\n",
    "\n",
    "# Answer 7\n",
    "\n",
    "# run predictor on all test examples\n",
    "yhat = predictor.predict(X_test)\n",
    "\n",
    "# find accuracy after running the predictor on all test examples\n",
    "print(accuracy_rate(yhat,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66\n"
     ]
    }
   ],
   "source": [
    "accuracy = predictor.score(X_test,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**. Train `predictor` using different choices of k. Try $k = 1, 3, 5, 15, 25, 50$. Report the accuracies on the test data (you can use the score method). Which choice of $k$ resulted in the highest accuracy? Comment briefly if the results make sense to you.\n",
    "\n",
    "**Question 9**. Other than choice of $k$, `KNeighborsClassifier` allows you to make some other choices. For example, in *Question 3* you saw that you can use a weighted prediction. There are few other options. Study the documentation and summarize in few sentences what other options you have when training the kNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k as: 1 we achieve accuracy rate of: 0.66\n",
      "With k as: 3 we achieve accuracy rate of: 0.72\n",
      "With k as: 5 we achieve accuracy rate of: 0.76\n",
      "With k as: 5 we achieve accuracy rate of: 0.76\n",
      "With k as: 15 we achieve accuracy rate of: 0.74\n",
      "With k as: 25 we achieve accuracy rate of: 0.66\n",
      "With k as: 50 we achieve accuracy rate of: 0.66\n"
     ]
    }
   ],
   "source": [
    "# Answer 8\n",
    "k_choices = [1,3,5,5,15,25,50]\n",
    "\n",
    "for k in k_choices:\n",
    "    predictor = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "    predictor.fit(X_train, y_train);\n",
    "    accuracy = predictor.score(X_test,y_test)\n",
    "    print(\"With k as: \" + str(k) + \" we achieve accuracy rate of: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8 (continued):**\n",
    "The choice of 5 for k resulted in the highest accuracy, which is 76%. \n",
    "\n",
    "In kNN algorithm, for k = 25 examples and the size of the training set is 66 examples, I think there is a possibility of overfitting here. Despite having the highest accuracy score on the testing set, the model was built on a relatively small dataset with somewhat a high choice for k.\n",
    "\n",
    "It is not surprising that if we set k too low or too high, the accuracy rates are low. If k is too low, then the model might not be able to observe some more farther neighbor datapoints that might have similar properties that might help to predict an example better.\n",
    "\n",
    "If k is too high, then the model might takes into consideration of even farther datapoints that have different properties that can skew the predictive label away from the true label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9:**\n",
    "The 3 options that I saw from the documentation are uniform, distance and [callable].\n",
    "\n",
    "For the uniform option, all of the examples are weighted the same.\n",
    "\n",
    "For the distance option, the closer neighbors are given greater weights than the further ones. \n",
    "\n",
    "For the [callable] option, the user can write a function that takes in an array of distances and return an array of the same shape that defines the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66\n",
      "0.66\n"
     ]
    }
   ],
   "source": [
    "# this customized function gives uniform weights to\n",
    "# all the examples\n",
    "def customized_weights(weights):\n",
    "    weight_new = []\n",
    "    for i in range(len(weights)):\n",
    "        weight_new.append(1)\n",
    "    return weight_new\n",
    "\n",
    "# using customized_ weights function \n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = k, weights=customized_weights)\n",
    "predictor.fit(X_train, y_train);\n",
    "print(predictor.score(X_test, y_test))\n",
    "\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = k, weights='uniform')\n",
    "predictor.fit(X_train, y_train);\n",
    "print(predictor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5.B Training kNN classifier on Iris and Wine Quality data (70% of grade)\n",
    "In this part of the lab you will use your knowledge to train and test accuracy of kNN classifiers on Iris and Newsgroups data.\n",
    "\n",
    "#### Iris Questions\n",
    "In Lab 7.A you used only the first two attributes of Iris for prediction. You have 2 questions:\n",
    "\n",
    "**Question Iris 1**. Train kNN classifier on other pair of attributes. Use $k$ of your choice and feel free to keep other choices at their default values. Which pair of attributes results in the highest accuracy?\n",
    "\n",
    "**Question Iris 2**. Train kNN classifier using all 4 attributes. Report the accuracy on test data set. Play with parameters of kNN to try to find a combination that results in the highest accuracy. Can you find something that works better than $k=3$ and default choices?\n",
    "\n",
    "#### Wine Quality Questions\n",
    "Wine Quality data set can be accessed from the UCI Machine Learning Repository website at https://archive.ics.uci.edu/ml/datasets/Wine+Quality. In this lab you will be working with the white wine data set https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
    "\n",
    "**Question Wine 1** Perform EDA on the data set to get an insight into the data. Prepare 1 page pdf document summarizing the interesting aspects of the data. Upload the pdf file named WineEDA.pdf to Canvas.\n",
    "\n",
    "**Question Wine 2**. Since kNN is a relatively slow algorithms, create your data set by picking 2,000 examples randomly. Pay attention that the 12th column in the data set (quality) should be your label (call it y) and the first 11 columns should be your attribues (call it X). Because y are numbers from 0-10, let us create a binary label by converting ratings 7 and below to 0 and ratings above 7 as 1. Split your data into 66% training and 33% test data sets. Train a kNN classifier ($k=3$ and defaults) on the training data and test its accuracy on test data. Record the computational time needed to run this. Report the accuracy.\n",
    "\n",
    "**Question Wine 3**. Train a kNN classifier using different values of $k$ and any other different choice of hyperparameters, hoping to improve accuracy. Report the results.\n",
    "\n",
    "**Question Wine 4** Pay attention that range of vallues for different attributes is quite different. *Question*: what is the impact of attributes with different ranges to kNN algorithm? In order to give each attribute equal change, scale each attribute to the same range to make sure the minimum is 0 and maximum is 1. *Question*: What is the formula to achieve this? Implement this formula and create a scaled data set (remember, any scaling you apply on the training data should also be used on the test data). Apply kNN and measure the accuracy. Did your accuracy improve?\n",
    "\n",
    "**Question Wine 5** Instead of treating the last column (quality) as a class label, we can treat is as numeric output. In the original format, the numbers range from 0 to 10. If we want to predict that number, the problem becomes regression. As we covered in class, kNN can also be used for regression. It is also implemented by sklearn. Use sklearn to test the accuracy of knn regression. Remember that the formula for regression accuracy is average squared error, also called the Mean Squared Error (MSE). \n",
    "\n",
    "**Deliverables** Submit the 1-page pdf file for EDA. Also, submit this notebook (enhanced with needed code) with all answers (added as tect comments) and all executed code for all the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Iris 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# training the model on the other pair of attributes\n",
    "X = iris.data[:, [2,3]] \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "predictor.fit(X_train, y_train);\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our observation, with k = 3 and 3rd with 4th attributes for training, I can see that the accuracy rate increased drastically. \n",
    "\n",
    "The pair of attributes that consist of 3rd and 4th attributes result in the highest accuracy rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Iris 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "# training the model on all 4 of the attributes\n",
    "X = iris.data \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "predictor.fit(X_train, y_train);\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 uniform euclidean 0.96\n",
      "1 uniform manhattan 0.96\n",
      "1 uniform chebyshev 1.0\n",
      "1 uniform minkowski 0.96\n",
      "1 distance euclidean 0.96\n",
      "1 distance manhattan 0.96\n",
      "1 distance chebyshev 1.0\n",
      "1 distance minkowski 0.96\n",
      "3 uniform euclidean 0.98\n",
      "3 uniform manhattan 0.98\n",
      "3 uniform chebyshev 0.98\n",
      "3 uniform minkowski 0.98\n",
      "3 distance euclidean 0.98\n",
      "3 distance manhattan 0.98\n",
      "3 distance chebyshev 0.98\n",
      "3 distance minkowski 0.98\n",
      "5 uniform euclidean 1.0\n",
      "5 uniform manhattan 0.98\n",
      "5 uniform chebyshev 1.0\n",
      "5 uniform minkowski 1.0\n",
      "5 distance euclidean 1.0\n",
      "5 distance manhattan 0.98\n",
      "5 distance chebyshev 1.0\n",
      "5 distance minkowski 1.0\n",
      "5 uniform euclidean 1.0\n",
      "5 uniform manhattan 0.98\n",
      "5 uniform chebyshev 1.0\n",
      "5 uniform minkowski 1.0\n",
      "5 distance euclidean 1.0\n",
      "5 distance manhattan 0.98\n",
      "5 distance chebyshev 1.0\n",
      "5 distance minkowski 1.0\n",
      "15 uniform euclidean 0.98\n",
      "15 uniform manhattan 0.98\n",
      "15 uniform chebyshev 0.94\n",
      "15 uniform minkowski 0.98\n",
      "15 distance euclidean 0.98\n",
      "15 distance manhattan 0.98\n",
      "15 distance chebyshev 0.96\n",
      "15 distance minkowski 0.98\n",
      "25 uniform euclidean 0.96\n",
      "25 uniform manhattan 0.94\n",
      "25 uniform chebyshev 0.94\n",
      "25 uniform minkowski 0.96\n",
      "25 distance euclidean 0.96\n",
      "25 distance manhattan 0.94\n",
      "25 distance chebyshev 0.96\n",
      "25 distance minkowski 0.96\n",
      "50 uniform euclidean 0.86\n",
      "50 uniform manhattan 0.92\n",
      "50 uniform chebyshev 0.86\n",
      "50 uniform minkowski 0.86\n",
      "50 distance euclidean 0.96\n",
      "50 distance manhattan 0.96\n",
      "50 distance chebyshev 0.98\n",
      "50 distance minkowski 0.96\n"
     ]
    }
   ],
   "source": [
    "# testing the different combinations of \n",
    "# k, weights and metrics\n",
    "k_choices = [1,3,5,5,15,25,50]\n",
    "weights = ['uniform', 'distance']\n",
    "metric_choices = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "dict_kNN = {}\n",
    "\n",
    "for k in k_choices:\n",
    "    for w in weights:\n",
    "        for m in metric_choices:\n",
    "            predictor = neighbors.KNeighborsClassifier(n_neighbors = k, metric=m, weights=w, p=2)\n",
    "            predictor.fit(X_train, y_train);\n",
    "            accuracy = predictor.score(X_test,y_test)\n",
    "            dict_kNN[(k, w, m)] = accuracy\n",
    "            print(k, w, m, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'uniform', 'chebyshev'), (1, 'distance', 'chebyshev'), (5, 'uniform', 'euclidean'), (5, 'uniform', 'chebyshev'), (5, 'uniform', 'minkowski'), (5, 'distance', 'euclidean'), (5, 'distance', 'chebyshev'), (5, 'distance', 'minkowski'), (3, 'uniform', 'euclidean'), (3, 'uniform', 'manhattan')]\n"
     ]
    }
   ],
   "source": [
    "# sorting the keys in dictionary to see which choices\n",
    "# for kNN params result in the highest accuracy rate\n",
    "\n",
    "# sorting the dictionary by values in descending order\n",
    "sorted_dict = [k for k, v in sorted(dict_kNN.items(), key=lambda item: item[1], reverse=True)]\n",
    "\n",
    "# print out the paramaters lists that gave the top 10 accuracy rate\n",
    "print(sorted_dict[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed above, with k = 1, the model resulted in very high accuracy rate. For parameters that result in top 10 accuracy rate, 7 out of 10 parameter list contains k = 1. In regards to the parameters of weights and distance, there seems to be no best choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Wine 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           float64\n",
       "volatile acidity        float64\n",
       "citric acid             float64\n",
       "residual sugar          float64\n",
       "chlorides               float64\n",
       "free sulfur dioxide     float64\n",
       "total sulfur dioxide    float64\n",
       "density                 float64\n",
       "pH                      float64\n",
       "sulphates               float64\n",
       "alcohol                 float64\n",
       "quality                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>6.854788</td>\n",
       "      <td>0.278241</td>\n",
       "      <td>0.334192</td>\n",
       "      <td>6.391415</td>\n",
       "      <td>0.045772</td>\n",
       "      <td>35.308085</td>\n",
       "      <td>138.360657</td>\n",
       "      <td>0.994027</td>\n",
       "      <td>3.188267</td>\n",
       "      <td>0.489847</td>\n",
       "      <td>10.514267</td>\n",
       "      <td>5.877909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.100795</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>5.072058</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>17.007137</td>\n",
       "      <td>42.498065</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>0.114126</td>\n",
       "      <td>1.230621</td>\n",
       "      <td>0.885639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.991723</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.993740</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
       "mean        6.854788          0.278241     0.334192        6.391415   \n",
       "std         0.843868          0.100795     0.121020        5.072058   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.320000        5.200000   \n",
       "75%         7.300000          0.320000     0.390000        9.900000   \n",
       "max        14.200000          1.100000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
       "mean      0.045772            35.308085            138.360657     0.994027   \n",
       "std       0.021848            17.007137             42.498065     0.002991   \n",
       "min       0.009000             2.000000              9.000000     0.987110   \n",
       "25%       0.036000            23.000000            108.000000     0.991723   \n",
       "50%       0.043000            34.000000            134.000000     0.993740   \n",
       "75%       0.050000            46.000000            167.000000     0.996100   \n",
       "max       0.346000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
       "mean      3.188267     0.489847    10.514267     5.877909  \n",
       "std       0.151001     0.114126     1.230621     0.885639  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.090000     0.410000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.550000    11.400000     6.000000  \n",
       "max       3.820000     1.080000    14.200000     9.000000  "
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average fixed acidity\n",
      "\n",
      "quality\n",
      "3    7.600000\n",
      "4    7.129448\n",
      "5    6.933974\n",
      "6    6.837671\n",
      "7    6.734716\n",
      "8    6.657143\n",
      "9    7.420000\n",
      "Name: fixed acidity, dtype: float64\n",
      "The average volatile acidity\n",
      "\n",
      "quality\n",
      "3    0.333250\n",
      "4    0.381227\n",
      "5    0.302011\n",
      "6    0.260564\n",
      "7    0.262767\n",
      "8    0.277400\n",
      "9    0.298000\n",
      "Name: volatile acidity, dtype: float64\n",
      "The average citric acid\n",
      "\n",
      "quality\n",
      "3    0.336000\n",
      "4    0.304233\n",
      "5    0.337653\n",
      "6    0.338025\n",
      "7    0.325625\n",
      "8    0.326514\n",
      "9    0.386000\n",
      "Name: citric acid, dtype: float64\n",
      "The average residual sugar\n",
      "\n",
      "quality\n",
      "3    6.392500\n",
      "4    4.628221\n",
      "5    7.334969\n",
      "6    6.441606\n",
      "7    5.186477\n",
      "8    5.671429\n",
      "9    4.120000\n",
      "Name: residual sugar, dtype: float64\n",
      "The average chlorides\n",
      "\n",
      "quality\n",
      "3    0.054300\n",
      "4    0.050098\n",
      "5    0.051546\n",
      "6    0.045217\n",
      "7    0.038191\n",
      "8    0.038314\n",
      "9    0.027400\n",
      "Name: chlorides, dtype: float64\n",
      "The average free sulfur dioxide\n",
      "\n",
      "quality\n",
      "3    53.325000\n",
      "4    23.358896\n",
      "5    36.432052\n",
      "6    35.650591\n",
      "7    34.125568\n",
      "8    36.720000\n",
      "9    33.400000\n",
      "Name: free sulfur dioxide, dtype: float64\n",
      "The average total sulfur dioxide\n",
      "\n",
      "quality\n",
      "3    170.600000\n",
      "4    125.279141\n",
      "5    150.904598\n",
      "6    137.047316\n",
      "7    125.114773\n",
      "8    126.165714\n",
      "9    116.000000\n",
      "Name: total sulfur dioxide, dtype: float64\n",
      "The average density\n",
      "\n",
      "quality\n",
      "3    0.994884\n",
      "4    0.994277\n",
      "5    0.995263\n",
      "6    0.993961\n",
      "7    0.992452\n",
      "8    0.992236\n",
      "9    0.991460\n",
      "Name: density, dtype: float64\n",
      "The average pH\n",
      "\n",
      "quality\n",
      "3    3.187500\n",
      "4    3.182883\n",
      "5    3.168833\n",
      "6    3.188599\n",
      "7    3.213898\n",
      "8    3.218686\n",
      "9    3.308000\n",
      "Name: pH, dtype: float64\n",
      "The average sulphates\n",
      "\n",
      "quality\n",
      "3    0.474500\n",
      "4    0.476135\n",
      "5    0.482203\n",
      "6    0.491106\n",
      "7    0.503102\n",
      "8    0.486229\n",
      "9    0.466000\n",
      "Name: sulphates, dtype: float64\n",
      "The average alcohol\n",
      "\n",
      "quality\n",
      "3    10.345000\n",
      "4    10.152454\n",
      "5     9.808840\n",
      "6    10.575372\n",
      "7    11.367936\n",
      "8    11.636000\n",
      "9    12.180000\n",
      "Name: alcohol, dtype: float64\n",
      "The average quality\n",
      "\n",
      "quality\n",
      "3    3\n",
      "4    4\n",
      "5    5\n",
      "6    6\n",
      "7    7\n",
      "8    8\n",
      "9    9\n",
      "Name: quality, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# split the data by quality\n",
    "# and perform aggregation analysis\n",
    "grouped_quality = wine.groupby('quality')\n",
    "\n",
    "for attribute in wine.columns:\n",
    "    avg = grouped_quality[attribute].agg('mean')\n",
    "    print(\"The average \" + attribute + \"\\n\")\n",
    "    print(str(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity          -0.113663\n",
       "volatile acidity       -0.194723\n",
       "citric acid            -0.009209\n",
       "residual sugar         -0.097577\n",
       "chlorides              -0.209934\n",
       "free sulfur dioxide     0.008158\n",
       "total sulfur dioxide   -0.174737\n",
       "density                -0.307123\n",
       "pH                      0.099427\n",
       "sulphates               0.053678\n",
       "alcohol                 0.435575\n",
       "quality                 1.000000\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the correlation coefficient between each attribute\n",
    "# and the quality of the wine\n",
    "wine.corr()['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1c75ce50>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "Var_Corr = wine.corr()\n",
    "sns.heatmap(Var_Corr, xticklabels=Var_Corr.columns, yticklabels=Var_Corr.columns, annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe from finding the correlation coefficient of each attribute to the quality of the wine, there seems to be no strong attribute that has a strong correlation of the quality of the wine because all of them have their correlation coefficient against the wine quality to be under 50%.\n",
    "\n",
    "In our observation of the heatmap, we do observe some high correlation among the attributes themselves.\n",
    "\n",
    "Density and Residual Sugar has a strong positive correlation coefficient.\n",
    "\n",
    "The following pairs of attributes have a strong negative correlation coefficient:\n",
    "\n",
    "pH and fixed acidity\n",
    "alcohol vs residual sugar, chlorides, total sulfur dioxide, density.\n",
    "\n",
    "By observing high correlation coefficients among the attributes themselves, maybe we can get rid or perform dimensionality reduction on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes wine to have high quality?\n",
    "\n",
    "Below, we will perform analysis on wine with quality of 8 and up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>6.678333</td>\n",
       "      <td>0.277972</td>\n",
       "      <td>0.328167</td>\n",
       "      <td>5.628333</td>\n",
       "      <td>0.038011</td>\n",
       "      <td>36.627778</td>\n",
       "      <td>125.883333</td>\n",
       "      <td>0.992214</td>\n",
       "      <td>3.221167</td>\n",
       "      <td>0.485667</td>\n",
       "      <td>11.651111</td>\n",
       "      <td>8.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.830647</td>\n",
       "      <td>0.106912</td>\n",
       "      <td>0.085688</td>\n",
       "      <td>4.248523</td>\n",
       "      <td>0.013150</td>\n",
       "      <td>16.110662</td>\n",
       "      <td>32.719653</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.151375</td>\n",
       "      <td>0.145702</td>\n",
       "      <td>1.274349</td>\n",
       "      <td>0.164794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.987130</td>\n",
       "      <td>2.940000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>2.075000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>102.750000</td>\n",
       "      <td>0.990260</td>\n",
       "      <td>3.127500</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.991620</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>8.150000</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>148.500000</td>\n",
       "      <td>0.993478</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>212.500000</td>\n",
       "      <td>1.000600</td>\n",
       "      <td>3.590000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count     180.000000        180.000000   180.000000      180.000000   \n",
       "mean        6.678333          0.277972     0.328167        5.628333   \n",
       "std         0.830647          0.106912     0.085688        4.248523   \n",
       "min         3.900000          0.120000     0.040000        0.800000   \n",
       "25%         6.200000          0.200000     0.280000        2.075000   \n",
       "50%         6.800000          0.260000     0.320000        4.300000   \n",
       "75%         7.300000          0.330000     0.360000        8.150000   \n",
       "max         9.100000          0.660000     0.740000       14.800000   \n",
       "\n",
       "        chlorides  free sulfur dioxide  total sulfur dioxide     density  \\\n",
       "count  180.000000           180.000000            180.000000  180.000000   \n",
       "mean     0.038011            36.627778            125.883333    0.992214   \n",
       "std      0.013150            16.110662             32.719653    0.002791   \n",
       "min      0.014000             6.000000             59.000000    0.987130   \n",
       "25%      0.030000            28.000000            102.750000    0.990260   \n",
       "50%      0.035500            34.500000            122.000000    0.991620   \n",
       "75%      0.044000            44.250000            148.500000    0.993478   \n",
       "max      0.121000           105.000000            212.500000    1.000600   \n",
       "\n",
       "               pH   sulphates     alcohol     quality  \n",
       "count  180.000000  180.000000  180.000000  180.000000  \n",
       "mean     3.221167    0.485667   11.651111    8.027778  \n",
       "std      0.151375    0.145702    1.274349    0.164794  \n",
       "min      2.940000    0.250000    8.500000    8.000000  \n",
       "25%      3.127500    0.380000   11.000000    8.000000  \n",
       "50%      3.230000    0.460000   12.000000    8.000000  \n",
       "75%      3.330000    0.582500   12.600000    8.000000  \n",
       "max      3.590000    0.950000   14.000000    9.000000  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_eight_and_up = wine[wine[\"quality\"] >= 8]\n",
    "wine_eight_and_up.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes wine to have low quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will perform analysis on wine with quality of 4 and down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>7.180874</td>\n",
       "      <td>0.375984</td>\n",
       "      <td>0.307705</td>\n",
       "      <td>4.821038</td>\n",
       "      <td>0.050557</td>\n",
       "      <td>26.633880</td>\n",
       "      <td>130.232240</td>\n",
       "      <td>0.994343</td>\n",
       "      <td>3.183388</td>\n",
       "      <td>0.475956</td>\n",
       "      <td>10.173497</td>\n",
       "      <td>3.890710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.171885</td>\n",
       "      <td>0.170525</td>\n",
       "      <td>0.157131</td>\n",
       "      <td>4.322845</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>31.001858</td>\n",
       "      <td>62.373163</td>\n",
       "      <td>0.002504</td>\n",
       "      <td>0.168668</td>\n",
       "      <td>0.117847</td>\n",
       "      <td>1.027570</td>\n",
       "      <td>0.312858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.989200</td>\n",
       "      <td>2.830000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>85.500000</td>\n",
       "      <td>0.992580</td>\n",
       "      <td>3.060000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>0.996010</td>\n",
       "      <td>3.285000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.000400</td>\n",
       "      <td>3.720000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count     183.000000        183.000000   183.000000      183.000000   \n",
       "mean        7.180874          0.375984     0.307705        4.821038   \n",
       "std         1.171885          0.170525     0.157131        4.322845   \n",
       "min         4.200000          0.110000     0.000000        0.700000   \n",
       "25%         6.400000          0.260000     0.205000        1.350000   \n",
       "50%         6.900000          0.320000     0.300000        2.700000   \n",
       "75%         7.650000          0.460000     0.400000        7.500000   \n",
       "max        11.800000          1.100000     0.880000       17.550000   \n",
       "\n",
       "        chlorides  free sulfur dioxide  total sulfur dioxide     density  \\\n",
       "count  183.000000           183.000000            183.000000  183.000000   \n",
       "mean     0.050557            26.633880            130.232240    0.994343   \n",
       "std      0.028700            31.001858             62.373163    0.002504   \n",
       "min      0.013000             3.000000             10.000000    0.989200   \n",
       "25%      0.037500             9.000000             85.500000    0.992580   \n",
       "50%      0.046000            18.000000            119.000000    0.994100   \n",
       "75%      0.054000            33.500000            177.000000    0.996010   \n",
       "max      0.290000           289.000000            440.000000    1.000400   \n",
       "\n",
       "               pH   sulphates     alcohol     quality  \n",
       "count  183.000000  183.000000  183.000000  183.000000  \n",
       "mean     3.183388    0.475956   10.173497    3.890710  \n",
       "std      0.168668    0.117847    1.027570    0.312858  \n",
       "min      2.830000    0.250000    8.000000    3.000000  \n",
       "25%      3.060000    0.380000    9.400000    4.000000  \n",
       "50%      3.160000    0.470000   10.100000    4.000000  \n",
       "75%      3.285000    0.540000   10.800000    4.000000  \n",
       "max      3.720000    0.870000   13.500000    4.000000  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_four_and_down = wine[wine[\"quality\"] <= 4]\n",
    "wine_four_and_down.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean difference in fixed acidity is: -0.502540983606556\n",
      "The mean difference in volatile acidity is: -0.09801138433515516\n",
      "The mean difference in citric acid is: 0.020461748633880006\n",
      "The mean difference in residual sugar is: 0.8072950819672116\n",
      "The mean difference in chlorides is: -0.012546265938069186\n",
      "The mean difference in free sulfur dioxide is: 9.993897996357013\n",
      "The mean difference in total sulfur dioxide is: -4.348907103825141\n",
      "The mean difference in density is: -0.002128671220399947\n",
      "The mean difference in pH is: 0.0377786885245901\n",
      "The mean difference in sulphates is: 0.009710382513660909\n",
      "The mean difference in alcohol is: 1.4776138433515449\n",
      "The mean difference in quality is: 4.1370673952641175\n"
     ]
    }
   ],
   "source": [
    "for attribute in wine.columns:\n",
    "    diff = wine_eight_and_up.describe().loc[\"mean\", attribute] - wine_four_and_down.describe().loc[\"mean\",attribute]\n",
    "    print(\"The mean difference in \" + str(attribute) + \" is: \" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Wine 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset by picking 2000 examples randomly\n",
    "wine_2000 = wine.sample(n = 2000)\n",
    "\n",
    "# turning y into a binary classifer\n",
    "# =< 7  rating is 0, else is 1\n",
    "wine_2000.loc[wine_2000['quality'] <= 7, \"quality\"] = 0\n",
    "wine_2000.loc[wine_2000['quality'] > 7, \"quality\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_2000.iloc[:, [i for i in range(11)]]\n",
    "y = wine_2000.iloc[:, [11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2836</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.053</td>\n",
       "      <td>52.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.48</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4089</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.24</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.098</td>\n",
       "      <td>36.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.99412</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.056</td>\n",
       "      <td>44.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.053</td>\n",
       "      <td>61.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0.99530</td>\n",
       "      <td>3.63</td>\n",
       "      <td>0.76</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.028</td>\n",
       "      <td>39.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99182</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.46</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4031</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.060</td>\n",
       "      <td>15.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.99240</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1655</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.046</td>\n",
       "      <td>27.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.99360</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.59</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2247</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.29</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.046</td>\n",
       "      <td>39.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.99471</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1342</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.27</td>\n",
       "      <td>12.15</td>\n",
       "      <td>0.033</td>\n",
       "      <td>37.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2909</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.031</td>\n",
       "      <td>23.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.99032</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.52</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1340 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "2836            6.5              0.22         0.45            8.00      0.053   \n",
       "4089            6.8              0.27         0.24            4.60      0.098   \n",
       "675             6.1              0.28         0.25            6.90      0.056   \n",
       "1575            6.8              0.17         0.74            2.40      0.053   \n",
       "2570            6.6              0.24         0.28            1.80      0.028   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4031            6.4              0.29         0.57            1.00      0.060   \n",
       "1655            7.8              0.28         0.49            1.30      0.046   \n",
       "2247            6.1              0.15         0.29            6.20      0.046   \n",
       "1342            8.4              0.58         0.27           12.15      0.033   \n",
       "2909            6.2              0.18         0.30            1.00      0.031   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "2836                 52.0                 196.0  0.99590  3.23       0.48   \n",
       "4089                 36.0                 127.0  0.99412  3.15       0.49   \n",
       "675                  44.0                 201.0  0.99550  3.19       0.40   \n",
       "1575                 61.0                 182.0  0.99530  3.63       0.76   \n",
       "2570                 39.0                 132.0  0.99182  3.34       0.46   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4031                 15.0                 120.0  0.99240  3.06       0.41   \n",
       "1655                 27.0                 142.0  0.99360  3.09       0.59   \n",
       "2247                 39.0                 151.0  0.99471  3.60       0.44   \n",
       "1342                 37.0                 116.0  0.99590  2.99       0.39   \n",
       "2909                 23.0                  73.0  0.99032  3.23       0.52   \n",
       "\n",
       "      alcohol  \n",
       "2836      9.1  \n",
       "4089      9.6  \n",
       "675       9.1  \n",
       "1575     10.5  \n",
       "2570     11.4  \n",
       "...       ...  \n",
       "4031      9.5  \n",
       "1655     10.2  \n",
       "2247     10.6  \n",
       "1342     10.8  \n",
       "2909     11.3  \n",
       "\n",
       "[1340 rows x 11 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the test set will be 33% of the original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696969696969697\n",
      "--- 0.15952205657958984 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# training the classifer with k = 3 and the default options\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "predictor.fit(X_train, y_train);\n",
    "\n",
    "print(predictor.score(X_test, y_test))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With k = 3, the accuracy rate is 96%, which is very impressive.\n",
    "\n",
    "The total computational time for a dataset of 2000 examples is .15 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Wine 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 uniform euclidean 0.9590909090909091\n",
      "1 uniform manhattan 0.9575757575757575\n",
      "1 uniform chebyshev 0.9575757575757575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 uniform minkowski 0.9590909090909091\n",
      "1 distance euclidean 0.9590909090909091\n",
      "1 distance manhattan 0.9575757575757575\n",
      "1 distance chebyshev 0.9575757575757575\n",
      "1 distance minkowski 0.9590909090909091\n",
      "3 uniform euclidean 0.9696969696969697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 uniform manhattan 0.9712121212121212\n",
      "3 uniform chebyshev 0.9666666666666667\n",
      "3 uniform minkowski 0.9696969696969697\n",
      "3 distance euclidean 0.9712121212121212\n",
      "3 distance manhattan 0.9742424242424242\n",
      "3 distance chebyshev 0.9696969696969697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 distance minkowski 0.9712121212121212\n",
      "5 uniform euclidean 0.9712121212121212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 uniform manhattan 0.9696969696969697\n",
      "5 uniform chebyshev 0.9712121212121212\n",
      "5 uniform minkowski 0.9712121212121212\n",
      "5 distance euclidean 0.9742424242424242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 distance manhattan 0.9742424242424242\n",
      "5 distance chebyshev 0.9772727272727273\n",
      "5 distance minkowski 0.9742424242424242\n",
      "5 uniform euclidean 0.9712121212121212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 uniform manhattan 0.9696969696969697\n",
      "5 uniform chebyshev 0.9712121212121212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 uniform minkowski 0.9712121212121212\n",
      "5 distance euclidean 0.9742424242424242\n",
      "5 distance manhattan 0.9742424242424242\n",
      "5 distance chebyshev 0.9772727272727273\n",
      "5 distance minkowski 0.9742424242424242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 uniform euclidean 0.9712121212121212\n",
      "15 uniform manhattan 0.9712121212121212\n",
      "15 uniform chebyshev 0.9712121212121212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 uniform minkowski 0.9712121212121212\n",
      "15 distance euclidean 0.9787878787878788\n",
      "15 distance manhattan 0.9787878787878788\n",
      "15 distance chebyshev 0.9787878787878788\n",
      "15 distance minkowski 0.9787878787878788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 uniform euclidean 0.9712121212121212\n",
      "25 uniform manhattan 0.9712121212121212\n",
      "25 uniform chebyshev 0.9712121212121212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 uniform minkowski 0.9712121212121212\n",
      "25 distance euclidean 0.9787878787878788\n",
      "25 distance manhattan 0.9787878787878788\n",
      "25 distance chebyshev 0.9787878787878788\n",
      "25 distance minkowski 0.9787878787878788\n",
      "50 uniform euclidean 0.9712121212121212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 uniform manhattan 0.9712121212121212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 uniform chebyshev 0.9712121212121212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 uniform minkowski 0.9712121212121212\n",
      "50 distance euclidean 0.9787878787878788\n",
      "50 distance manhattan 0.9787878787878788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 distance chebyshev 0.9787878787878788\n",
      "50 distance minkowski 0.9787878787878788\n",
      "[(15, 'distance', 'euclidean'), (15, 'distance', 'manhattan'), (15, 'distance', 'chebyshev'), (15, 'distance', 'minkowski'), (25, 'distance', 'euclidean'), (25, 'distance', 'manhattan'), (25, 'distance', 'chebyshev'), (25, 'distance', 'minkowski'), (50, 'distance', 'euclidean'), (50, 'distance', 'manhattan')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "k_choices = [1,3,5,5,15,25,50]\n",
    "weights = ['uniform', 'distance']\n",
    "metric_choices = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "dict_kNN = {}\n",
    "\n",
    "for k in k_choices:\n",
    "    for w in weights:\n",
    "        for m in metric_choices:\n",
    "            predictor = neighbors.KNeighborsClassifier(n_neighbors = k, metric=m, weights=w, p=2)\n",
    "            predictor.fit(X_train, y_train);\n",
    "            accuracy = predictor.score(X_test,y_test)\n",
    "            dict_kNN[(k, w, m)] = accuracy\n",
    "            print(k, w, m, accuracy)\n",
    "# sorting the dictionary by values in descending order\n",
    "sorted_dict = [k for k, v in sorted(dict_kNN.items(), key=lambda item: item[1], reverse=True)]\n",
    "\n",
    "# print out the paramaters lists that gave the top 10 accuracy rate\n",
    "print(sorted_dict[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameter list yields the highest accuracy rate:\n",
    "\n",
    "(5, 'distance', 'chebyshev'), \n",
    "\n",
    "(5, 'distance', 'euclidean'), \n",
    "\n",
    "(5, 'distance', 'minkowski'), \n",
    "\n",
    "(15, 'distance', 'euclidean'), \n",
    "\n",
    "(15, 'distance', 'manhattan'), \n",
    "\n",
    "(15, 'distance', 'chebyshev'), \n",
    "\n",
    "(15, 'distance', 'minkowski'), \n",
    "\n",
    "(25, 'distance', 'euclidean'), \n",
    "\n",
    "(25, 'distance', 'manhattan'), \n",
    "\n",
    "(25, 'distance', 'chebyshev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params  (15, 'distance', 'euclidean') has accuracy rate:  0.9787878787878788\n",
      "Params  (15, 'distance', 'manhattan') has accuracy rate:  0.9787878787878788\n",
      "Params  (15, 'distance', 'chebyshev') has accuracy rate:  0.9787878787878788\n",
      "Params  (15, 'distance', 'minkowski') has accuracy rate:  0.9787878787878788\n",
      "Params  (25, 'distance', 'euclidean') has accuracy rate:  0.9787878787878788\n",
      "Params  (25, 'distance', 'manhattan') has accuracy rate:  0.9787878787878788\n",
      "Params  (25, 'distance', 'chebyshev') has accuracy rate:  0.9787878787878788\n",
      "Params  (25, 'distance', 'minkowski') has accuracy rate:  0.9787878787878788\n",
      "Params  (50, 'distance', 'euclidean') has accuracy rate:  0.9787878787878788\n",
      "Params  (50, 'distance', 'manhattan') has accuracy rate:  0.9787878787878788\n"
     ]
    }
   ],
   "source": [
    "for k in sorted_dict[:10]:\n",
    "    print(\"Params \", k, \"has accuracy rate: \", dict_kNN[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Wine 4:**\n",
    "The formula for normalizing the data in which the minum is 0 and the maximum is 1 is:\n",
    "\n",
    "normalized_data = (x - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2836</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.290766</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.399015</td>\n",
       "      <td>0.546040</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4089</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.157171</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.112281</td>\n",
       "      <td>0.229064</td>\n",
       "      <td>0.431423</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.313953</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.247544</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.411330</td>\n",
       "      <td>0.520283</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.070727</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.364532</td>\n",
       "      <td>0.507405</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.047151</td>\n",
       "      <td>0.050725</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.283323</td>\n",
       "      <td>0.561224</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2394</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.723529</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.483301</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.536946</td>\n",
       "      <td>0.685126</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.313953</td>\n",
       "      <td>0.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.522593</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.224561</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.623310</td>\n",
       "      <td>0.581633</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4210</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.212181</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.077193</td>\n",
       "      <td>0.246305</td>\n",
       "      <td>0.417257</td>\n",
       "      <td>0.295918</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3628</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.184676</td>\n",
       "      <td>0.373188</td>\n",
       "      <td>0.185965</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.461687</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.244186</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4583</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.286837</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.196491</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.480361</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.313953</td>\n",
       "      <td>0.216667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "2836       0.214286          0.164706         0.45        0.290766   0.141304   \n",
       "4089       0.244898          0.223529         0.24        0.157171   0.304348   \n",
       "675        0.173469          0.235294         0.25        0.247544   0.152174   \n",
       "1575       0.244898          0.105882         0.74        0.070727   0.141304   \n",
       "2570       0.224490          0.188235         0.28        0.047151   0.050725   \n",
       "2394       0.193878          0.723529         0.55        0.483301   0.152174   \n",
       "642        0.193878          0.152941         0.33        0.522593   0.115942   \n",
       "4210       0.163265          0.105882         0.21        0.212181   0.130435   \n",
       "3628       0.204082          0.352941         0.20        0.184676   0.373188   \n",
       "4583       0.204082          0.176471         0.37        0.286837   0.130435   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "2836             0.168421              0.399015  0.546040  0.448980   \n",
       "4089             0.112281              0.229064  0.431423  0.367347   \n",
       "675              0.140351              0.411330  0.520283  0.408163   \n",
       "1575             0.200000              0.364532  0.507405  0.857143   \n",
       "2570             0.122807              0.241379  0.283323  0.561224   \n",
       "2394             0.189474              0.536946  0.685126  0.510204   \n",
       "642              0.224561              0.357143  0.623310  0.581633   \n",
       "4210             0.077193              0.246305  0.417257  0.295918   \n",
       "3628             0.185965              0.362069  0.461687  0.459184   \n",
       "4583             0.196491              0.285714  0.480361  0.071429   \n",
       "\n",
       "      sulphates   alcohol  \n",
       "2836   0.302326  0.183333  \n",
       "4089   0.313953  0.266667  \n",
       "675    0.209302  0.183333  \n",
       "1575   0.627907  0.416667  \n",
       "2570   0.279070  0.566667  \n",
       "2394   0.313953  0.116667  \n",
       "642    0.325581  0.400000  \n",
       "4210   0.372093  0.300000  \n",
       "3628   0.244186  0.250000  \n",
       "4583   0.313953  0.216667  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "X_train = normalize(X_train)\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9712121212121212\n"
     ]
    }
   ],
   "source": [
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "predictor.fit(X_train, y_train);\n",
    "\n",
    "print(predictor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the accuracy result of non-normalized vs. normalized datasets, I have only observe an increase of almost .03% in accuracy rate for min-max normalized dataset.\n",
    "\n",
    "Score of non-normalized: 0.9575757575757575\n",
    "\n",
    "Score of normalized: 0.9606060606060606"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Wine 5:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE value for k =  1 is: 0.2367712103711219\n",
      "RMSE value for k =  3 is: 0.20719385350268701\n",
      "RMSE value for k =  5 is: 0.19617555526835775\n",
      "RMSE value for k =  5 is: 0.19617555526835775\n",
      "RMSE value for k =  15 is: 0.18408020503444114\n",
      "RMSE value for k =  25 is: 0.1832170430076985\n",
      "RMSE value for k =  50 is: 0.1830913002914782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "wine_2000_no_bin_classifer = wine.sample(n = 2000)\n",
    "X = wine_2000.iloc[:, [i for i in range(11)]]\n",
    "y = wine_2000.iloc[:, [11]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "\n",
    "k_choices = [1,3,5,5,15,25,50]\n",
    "\n",
    "for k in k_choices: \n",
    "    predictor = neighbors.KNeighborsRegressor(n_neighbors = k)\n",
    "    predictor.fit(X_train, y_train);\n",
    "    pred = predictor.predict(X_test)\n",
    "    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n",
    "    print('RMSE value for k = ' , k , 'is:', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observing the k values for KNN Regressor, we see that the RMSE decreases as we increase k."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
